# Copyright 2025 Proyectos y Sistemas de Mantenimiento SL (eProsima).
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Any, Dict, Optional, Tuple

from vulcanai.core.executor import Blackboard, PlanExecutor
from vulcanai.core.llm_agent import LLMAgent
from vulcanai.console.logger import VulcanAILogger
from vulcanai.core.plan_types import GlobalPlan
from vulcanai.tools.tool_registry import ToolRegistry


class ToolManager:
    """Manages the LLM Agent and calls the executor with the LLM output."""
    def __init__(self, model: str, registry: Optional[ToolRegistry]=None, k: int=10, hist_depth: int = 3, logger=None):
        self.logger = logger or VulcanAILogger().log_manager
        self.llm = LLMAgent(model, self.logger)
        self.k = k
        self.registry = registry or ToolRegistry(logger=(logger or VulcanAILogger().log_registry))
        # self.validator = PlanValidator(registry)
        self.executor = PlanExecutor(self.registry, logger=(logger or VulcanAILogger().log_executor))
        self.bb = Blackboard()
        self.user_context = ""
        # History is saved as a list of Tuples of user requests and plan summaries
        self.history = []
        # How many previous interactions to include in the prompt as history
        self.history_depth = hist_depth

    def register_tool(self, tool, solve_deps: bool = True):
        """
        Wrapper for registering a single tool.

        :param tool: The tool class (ITool) to register.
        """
        self.registry.register_tool(tool, solve_deps=solve_deps)

    def register_tools_from_file(self, path: str):
        """
        Wrapper for discovering tools from a file.

        :param path: The absolute path to the file containing tool definitions.
        """
        self.registry.discover_tools_from_file(path)

    def register_tools_from_entry_points(self, group: str = "custom_tools"):
        """
        Wrapper for discovering tools from entry points.

        :param group: The entry point group name. Default is "custom_tools".
        """
        self.registry.discover_tools_from_entry_points(group)

    def add_user_context(self, context: str):
        """
        Add additional context to be included in the prompt.

        :param context: The context string to add.
        """
        self.user_context = context

    def handle_user_request(self, user_text: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Given a natural language request, ask LLM to generate a plan.
        Then execute it via PlanExecutor.

        :param user_text: The user request in natural language.
        :param context: Additional context that may help the LLM to choose the best tool.
        :return: A dictionary with the execution result, including the plan used and the final blackboard state.
        """
        try:
            # Get plan from LLM
            plan = self.get_plan_from_user_request(user_text, context)
            if not plan:
                return {"error": "No plan generated by LLM."}
            # Execute plan
            ret = self.execute_plan(plan)
        except Exception as e:
            self.logger(f"Error handling user request: {e}", error=True)
            ret = {"error": str(e)}

        return ret

    def get_plan_from_user_request(self, user_text: str, context: Dict[str, Any] = None) -> GlobalPlan:
        """
        Given a natural language request, ask LLM to generate a plan.

        :param user_text: The user request in natural language.
        :param context: Additional context that may help the LLM to choose the best tool.
        :return: A dictionary with the execution result, including the plan used and the final blackboard state.
        :raises Exception: If there is an error during LLM inference.
        """
        # Build prompt with available tools
        system_prompt, user_prompt = self._build_prompt(user_text, context)

        if not system_prompt or not user_prompt:
            return {}

        if context and "images" in context:
            # Images should be a list of paths
            images = context["images"]

        # Query LLM
        plan = self.llm.inference(system_prompt, user_prompt, images, self.history)
        self.logger(f"Plan received:\n{plan}")
        # Save to history
        if plan:
            self._add_to_history(user_prompt, plan.summary)
        return plan

    def execute_plan(self, plan: GlobalPlan) -> Dict[str, Any]:
        """
        Execute a given plan via PlanExecutor.

        :param plan: The plan to execute.
        :return: A dictionary with the execution result, including the final blackboard state.
        """
        result = self.executor.run(plan, self.bb)
        return {"plan": plan, **result}

    def _parse_user_context(self) -> str:
        """
        Parse the user context into a string format suitable for the prompt.

        :param context: The user context in string format.
        :return: A formatted string representing the user context.
        """
        if not self.user_context:
            return ""
        return f"\n## User context:\n{self.user_context}\n"

    def _build_prompt(self, user_text: str, ctx: Dict[str, Any]) -> Tuple[str, str]:
        """
        Create a simple prompt listing available tools and asking for one.
        The prompt is divided into 'system' and 'user' parts, which will be handled by the LLM agent
        to create the most appropriate prompt for the specific LLM.
        """
        tools = self.registry.top_k(user_text, self.k)
        if not tools:
            self.logger("No tools available in the registry.", error=True)
            return "", ""
        tool_descriptions = []
        for tool in tools:
            tool_descriptions.append(
                f"- *{tool.name}*: {tool.description}\n"
                f"  Inputs: {tool.input_schema}\n"
                f"  Outputs: {tool.output_schema}\n"
            )
        tools_text = "\n".join(tool_descriptions)
        user_context = self._parse_user_context()
        user_prompt = "User request:\n" + user_text

        return self._get_prompt_template().format(tools_text=tools_text, user_context=user_context), user_prompt

    def _add_to_history(self, user_text: str, plan_summary: str):
        """Add a new interaction to the history and trim if necessary."""
        self.history.append((user_text, plan_summary))
        # Keep only the last `history_depth` interactions
        if len(self.history) > self.history_depth:
            if self.history_depth <= 0:
                self.history = []
            else:
                self.history = self.history[-self.history_depth:]

    def update_history_depth(self, new_depth: int):
        """
        Update the history depth and trim the history if necessary.

        :param new_depth: The new history depth.
        """
        self.history_depth = max(0, int(new_depth))
        self.logger(f"Updated history depth to {new_depth}")
        if len(self.history) > self.history_depth:
            if self.history_depth <= 0:
                self.history = []
            else:
                self.history = self.history[-self.history_depth:]

    def _get_prompt_template(self) -> str:
        template = """
You are a planner assistant controlling a robotic system.
Your job is to take a user request and generate a valid execution plan, containing only ONE step.
Be sure to understand the text received and select the best action command from the available options.
{user_context}
## Available tools:
{tools_text}

## Plan format:

plan = GlobalPlan(
    plan=[
        PlanNode(
            kind="SEQUENCE | PARALLEL",
            steps=[
                Step(tool="tool_name", args=[ArgValue(key="arg_name", val="value or {{{{bb.tool.key}}}}"), ...]),
            ],
            ## Optional execution control parameters:
            condition="Python expression to evaluate before executing this PlanNode",
            success_criteria="Python expression to determine if this PlanNode succeeded",
            timeout_ms=0,
            retry=0,
            on_fail=PlanNode(
                kind="SEQUENCE | PARALLEL",
                steps=[
                    Step(tool="tool_name", args=[ArgValue(key="arg_name", val="value or {{{{bb.tool.key}}}}"), ...]),
                ],
            ),
        )
    ],
)

Use "{{{{bb.tool.key}}}}" to reference the output of a previous step.
For example, if tool 'detect_object' outputs {{"pose": [1.0, 2.0]}}, you can pass it to navigate as:
"args": {{"target": "{{{{bb.detect_object.pose}}}}"}}

Choose the most appropriate tool and arguments to satisfy the request.
Add only optional execution control parameters if strictly necessary or requested by the user.
"""
        return template
